{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler, LabelBinarizer\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.svm import SVC, SVR, LinearSVR, LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, cross_validate, cross_val_predict, cross_val_score, train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "from sklearn.metrics import classification_report, make_scorer, accuracy_score, f1_score \n",
    "from sklearn.utils import resample\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42 \n",
    "# select computationally feasible options. As working with constraint resources, \n",
    "# random search is more likely to find one of the best specifications.\n",
    "N_FOLDS = 5 # 5\n",
    "N_RANDOMSEARCH = 75 # 75\n",
    "# run with -1 to utilize all resources (outside of notebook as it needs protection by a \n",
    "# if __name__ == \"__main__\" clause if run on windows)\n",
    "N_JOBS = None\n",
    "\n",
    "# load previous results?\n",
    "load_file = 'A4_results_new.p'\n",
    "\n",
    "DATA_PATH = '../data/'\n",
    "RESULTS_PATH = '../results/'\n",
    "pd.options.display.float_format = '{:.3f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wine = pd.read_csv(DATA_PATH + 'wines_transformed.csv')\n",
    "\n",
    "y = wine['quality']\n",
    "x = wine.drop(['quality', 'red'], axis=1)\n",
    "x_inclRed = wine.drop(['quality'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "Set up models and hyperparameter space.  \n",
    "We were instructed to treat it as a classification and regression task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "models['RandomForest'] = {'classifier': RandomForestClassifier(),\n",
    "                         'regressor': RandomForestRegressor(),\n",
    "                         'standardise': False,\n",
    "                         'clf_1hot': False,\n",
    "                         'param_grid': \n",
    "                              {'RandomForest__n_estimators': [10, 50, 100],\n",
    "                               'RandomForest__max_features': ['auto', 'sqrt'],\n",
    "                               'RandomForest__max_depth': scipy.stats.randint(10,100),\n",
    "                               'RandomForest__min_samples_split':  scipy.stats.randint(2,12),\n",
    "                               'RandomForest__min_samples_leaf': scipy.stats.randint(1,5),\n",
    "                               'RandomForest__class_weight': [None, 'balanced']\n",
    "                              }}\n",
    "\n",
    "models['LogisticReg'] = {'classifier': LogisticRegression(),\n",
    "                         'regressor': None,\n",
    "                         'standardise': True,\n",
    "                         'clf_1hot': False,\n",
    "                         'param_grid': \n",
    "                              {\n",
    "                                  'LogisticReg__class_weight': [None, 'balanced']\n",
    "                              }}\n",
    "models['LinearReg'] = {'classifier': None,\n",
    "                         'regressor': LinearRegression(),\n",
    "                         'standardise': True,\n",
    "                         'clf_1hot': False,\n",
    "                         'param_grid': \n",
    "                              {\n",
    "                                  'LinearReg__class_weight': [None, 'balanced']\n",
    "                              }}\n",
    "models['NaiveBayes'] = {'classifier': GaussianNB(),\n",
    "                         'regressor': None,\n",
    "                         'standardise': True,\n",
    "                         'clf_1hot': False,\n",
    "                         'param_grid': \n",
    "                              {}}                      \n",
    "models['KNN'] = {'classifier': KNeighborsClassifier(),\n",
    "                         'regressor': KNeighborsRegressor(),\n",
    "                         'standardise': True,\n",
    "                         'clf_1hot': False,\n",
    "                         'param_grid': \n",
    "                              {'KNN__n_neighbors': scipy.stats.randint(1,10),\n",
    "                               'KNN__weights': ['uniform' , 'distance']}}\n",
    "\n",
    "# define tuples for 2 or! 3 layers of 64 or 128 hidden units\n",
    "layer_sizes = [[l1, l2, l3] for l1 in [64,128] for l2 in [64,128] for l3 in [None,64,128]]\n",
    "[l.pop(-1) for l in layer_sizes if l[-1]==None]\n",
    "models['MLP'] = {'classifier': MLPClassifier(early_stopping=True), # reduce runtime\n",
    "                'regressor': MLPRegressor(early_stopping=True),\n",
    "                'standardise': True,\n",
    "                'clf_1hot': False,\n",
    "                'param_grid': \n",
    "                      {'MLP__hidden_layer_sizes': layer_sizes,\n",
    "                       'MLP__alpha':  scipy.stats.uniform(10**(-4), 10**3),\n",
    "                      }}\n",
    "models['Dummy_strat'] = {'classifier': DummyClassifier(strategy='stratified'), # default\n",
    "                         'regressor': DummyRegressor(strategy='mean'), # default\n",
    "                         'standardise': True,\n",
    "                         'clf_1hot': False,\n",
    "                         'param_grid': \n",
    "                             {}}\n",
    "models['Dummy_majority'] = {'classifier': DummyClassifier(strategy='most_frequent'),\n",
    "                         'regressor': DummyRegressor(strategy='mean'), # default\n",
    "                         'standardise': True,\n",
    "                         'clf_1hot': False,\n",
    "                         'param_grid': \n",
    "                             {}}\n",
    "\n",
    "# split SVM to speed up as certain paramters are kernel-specific \n",
    "cache_size = 5000 #in MB, speed-up if enough RAM (default: 200)\n",
    "C = np.logspace(-5, 15, num=30, base=2)\n",
    "models['SVM_rbf'] = {'classifier': SVC(kernel='rbf', cache_size=cache_size),\n",
    "            'regressor': SVR(kernel='rbf', cache_size=cache_size),\n",
    "            'standardise': True,\n",
    "            'clf_1hot': False,\n",
    "            'param_grid': \n",
    "                  {'SVM_rbf__C': C,\n",
    "                   'SVM_rbf__gamma': np.logspace(-15, 3, num=19, base=2), # only used by rbf\n",
    "                   'SVM_rbf__class_weight': [None, 'balanced']\n",
    "            }}\n",
    "\n",
    "# forgo polynomial kernel as runtime seems really exzessive\n",
    "#models['SVM_poly'] = {'classifier': SVC(kernel='poly', cache_size=cache_size),\n",
    "#                'regressor': SVR(kernel='poly', cache_size=cache_size),\n",
    "#                'standardise': False,\n",
    "#                'clf_1hot': False,\n",
    "#                'param_grid': \n",
    "#                      {'SVM_poly__C': C,\n",
    "#                       'SVM_poly__degree': [2,3,4], # only used by poly\n",
    "#                       'SVM_poly__class_weight': [None, 'balanced']}}\n",
    "\n",
    "# more efficient than SVC / SVR. \n",
    "# Allows to solve the primal problem, as #obs >> #features in this case\n",
    "# l2 loss, needed if dual=false\n",
    "models['SVM_linear'] = {'classifier': LinearSVC(dual=False),\n",
    "                'regressor': LinearSVR(dual=False, loss='squared_epsilon_insensitive'),\n",
    "                'standardise': True,\n",
    "                'clf_1hot': False,\n",
    "                'param_grid': \n",
    "                      {'SVM_linear__C': np.logspace(-5, 15, num=N_RANDOMSEARCH, base=2), # hyperparameter space needs to be large enough\n",
    "                       'SVM_linear__class_weight': [None, 'balanced']\n",
    "                        }}\n",
    "\n",
    "\n",
    "# define new scores for variance of the error (used for CIs) to not have to \n",
    "# explicitely obtain predictions via use cross_val_predict\n",
    "def mean_variance_mse(y, y_pred, **kwargs):\n",
    "    SE = (y - y_pred)**2\n",
    "    SE = SE.astype(np.float128) # seems like sklean switches to pd.var which doesn't support dtype argument and uses ddof=1. So better make sure\n",
    "    v = np.var(SE, ddof=0) / (SE.shape[0] - 1)\n",
    "    return v \n",
    "\n",
    "def mean_variance_acc(y, y_pred, **kwargs):\n",
    "    acc = (y == y_pred)\n",
    "    acc = acc.astype(np.float128) \n",
    "    v = np.var(acc, ddof=0) / (acc.shape[0] - 1)\n",
    "    return v\n",
    "\n",
    "def mean_variance_acc_reg(y, y_pred, **kwargs):\n",
    "    preds = np.round(y_pred, 0).astype(int)\n",
    "    acc = (y.astype(int) == preds)\n",
    "    acc = acc.astype(np.float128) \n",
    "    v = np.var(acc, ddof=0) / (acc.shape[0] - 1)\n",
    "    return v\n",
    "\n",
    "def regression_acc(y, y_pred, **kwargs):\n",
    "    preds = np.round(y_pred, 0).astype(int)\n",
    "    return (accuracy_score(y.astype(int), preds))\n",
    "\n",
    "def regression_f1_macro(y, y_pred, **kwargs):\n",
    "    preds = np.round(y_pred, 0).astype(int)\n",
    "    return f1_score(y.astype(int), preds, average='macro')\n",
    "\n",
    "mean_variance_score_mse = make_scorer(mean_variance_mse, greater_is_better=False)\n",
    "mean_variance_score_acc = make_scorer(mean_variance_acc, greater_is_better=True)\n",
    "mean_variance_score_acc_reg = make_scorer(mean_variance_acc_reg, greater_is_better=True)\n",
    "regression_acc_score =    make_scorer(regression_acc, greater_is_better=True)\n",
    "regression_f1_macro_score = make_scorer(regression_f1_macro, greater_is_better=True)\n",
    "\n",
    "# differentiate between classification and regression\n",
    "scoring_reg = {'mse': 'neg_mean_squared_error',\n",
    "               'mae': 'neg_mean_absolute_error',\n",
    "               'acc_reg': regression_acc_score,\n",
    "               'f1_macro': regression_f1_macro_score,\n",
    "               'var_mse': mean_variance_score_mse,\n",
    "               'var_acc': mean_variance_score_acc_reg}\n",
    "scoring_clf = {'acc': 'accuracy',\n",
    "               'precision_macro': 'precision_macro', # micro: calculated globally\n",
    "               'precision_weighted': 'precision_weighted',\n",
    "               'recall_macro': 'recall_macro',\n",
    "               'recall_weighted': 'recall_weighted',\n",
    "               'f1_micro': 'f1_micro',\n",
    "               'f1_macro': 'f1_macro',\n",
    "               'f1_weighted': 'f1_weighted',\n",
    "               'var_acc': mean_variance_score_acc}\n",
    "prefixes = ['x_', 'x_inclRed_']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a valid model validation setup:  \n",
    "An inner loop to tune the models and an outer loop to obtain the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inner_cv = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=random_state)\n",
    "outer_cv = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=random_state+1)\n",
    "\n",
    "for idx_data, X in enumerate([x, x_inclRed]): \n",
    "    print('\\nSTART NEW DATASET-LOOP\\n')\n",
    "    prefix = prefixes[idx_data]\n",
    "    \n",
    "    for classification in [True, False]:\n",
    "        if classification:\n",
    "            estimator = 'classifier'\n",
    "            scoring = scoring_clf\n",
    "            suffix = '_clf'\n",
    "        else:\n",
    "            estimator = 'regressor'\n",
    "            scoring = scoring_reg\n",
    "            suffix = '_reg'\n",
    "\n",
    "        for model, d in models.items():\n",
    "            print('CURRENT MODEL:', model, '- classification', classification)\n",
    "            if d[estimator] == None:\n",
    "                print('No estimator')\n",
    "                continue\n",
    "\n",
    "            # never actually needed, turns out sklearn always handles this\n",
    "            if classification and d['clf_1hot']:\n",
    "                lb = LabelBinarizer(sparse_output=False)\n",
    "                Y = lb.fit_transform(y)\n",
    "            else:\n",
    "                Y = y\n",
    "\n",
    "            pipe = Pipeline([\n",
    "                        ('scaler', RobustScaler(with_centering=d['standardise'], with_scaling=d['standardise'])),\n",
    "                        (model, d[estimator])\n",
    "            ])\n",
    "\n",
    "            # CV\n",
    "            grid = d['param_grid'].copy()\n",
    "            \n",
    "            # Benchmarks without hyperparameters: only run 1 iteration\n",
    "            if grid == {}:\n",
    "                iters = 1\n",
    "            else:\n",
    "                iters = N_RANDOMSEARCH\n",
    "               \n",
    "            gs = RandomizedSearchCV(pipe, grid, cv=inner_cv, n_iter=iters, n_jobs=N_JOBS)\n",
    "            score = cross_validate(gs, X=X, y=Y, cv=outer_cv, scoring=scoring, return_train_score=False)\n",
    "\n",
    "            d[prefix + 'score' + suffix] = score\n",
    "\n",
    "            if classification:\n",
    "                print('Acc: {:.3f}'.format(np.mean(score['test_acc'])))\n",
    "                #print(classification_report(y, preds))\n",
    "            else: \n",
    "                print('MSE: {:.3f}'.format(- np.mean(score['test_mse'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The given task included exploring four options:  \n",
    "    - Run it as classification and regression\n",
    "    - Does wine color add additional information, given the covariates?\n",
    "The combination of this results in four tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display helpers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(models, name, prettify):\n",
    "    '''\n",
    "    prettify:\n",
    "        'std': add +_196 * std of the scores, this is only to get an idea of the variance, \n",
    "        not in any way a valid confidence interval (correlated runs and way too few observations)!\n",
    "        'regression': create pretty table for regression results\n",
    "        'classification': for classification results\n",
    "    '''\n",
    "    results = {}\n",
    "    for model in models.keys():\n",
    "        try:\n",
    "            tmp = models[model][name].copy()\n",
    "            for k, v in tmp.items():\n",
    "                # np.abs() because 'mse' and 'mae' are defined as their negative\n",
    "                if 'var_acc' in k:\n",
    "                    if 'test_acc' in tmp.keys():\n",
    "                        acc_var = 'test_acc'\n",
    "                    elif 'test_acc_reg' in tmp.keys():\n",
    "                        acc_var = 'test_acc_reg'\n",
    "                    tmp[k] = '{:.3f} $\\pm$ {:.3f}'.format(np.abs(np.mean(models[model][name][acc_var])), \n",
    "                                                              1.96 * np.sqrt(np.mean(v)))\n",
    "                    \n",
    "                elif 'var_mse' in k:\n",
    "                    tmp[k] = '{:.3f} $\\pm$ {:.3f}'.format(np.abs(np.mean(models[model][name]['test_mse'])), \n",
    "                                                          1.96 * np.sqrt(-np.mean(v)))\n",
    "                else:\n",
    "                    if prettify == 'std':\n",
    "                        tmp[k] = '{:.3f} $\\pm$ {:.3f}'.format(np.abs(np.mean(v)), 1.96*np.std(v))\n",
    "                    else: \n",
    "                        tmp[k] = np.abs(np.mean(v))\n",
    "            results[model] = tmp\n",
    "        except:\n",
    "            print('no results:', model)\n",
    "            \n",
    "    df = pd.DataFrame(results)\n",
    "            \n",
    "    if prettify == 'regression':\n",
    "        df = prettify_table(df, model=prettify)\n",
    "    elif prettify == 'classification':\n",
    "        df = prettify_table(df, model=prettify)\n",
    "            \n",
    "    return df\n",
    "\n",
    "def prettify_table(df, model):  \n",
    "    # scores listed here will be contained in output, all others ignored\n",
    "    name_changes_idx = {\n",
    "        #'test_f1_micro': 'F1-score, micro',\n",
    "        'test_f1_macro': 'F1-score, macro',\n",
    "        #'test_f1_weighted': 'F1-score, weighted',\n",
    "        'test_precision_macro': 'Precision, macro',\n",
    "        #'test_precision_weighted': 'Precision, weighted',\n",
    "        'test_recall_macro': 'Recall, macro',\n",
    "        #'test_recall_weighted': 'Recall, weighted',\n",
    "        'test_var_acc': 'Accuracy',\n",
    "        \n",
    "        #'test_acc_reg': 'Accuracy',\n",
    "        'test_mae': 'MAE',\n",
    "        'test_var_mse': 'MSE'\n",
    "    }\n",
    "    drops = [name for name in df.index if name not in name_changes_idx.keys()]\n",
    "    df = df.drop(drops, axis=0)\n",
    "    \n",
    "    df.index = [name_changes_idx[name] for name in df.index]\n",
    "    \n",
    "    if model=='regression': \n",
    "        dummy = 'Mean'\n",
    "    elif model=='classification': \n",
    "        dummy = 'Majority'\n",
    "    name_changes_cols = {\n",
    "        'Dummy_majority': dummy + ' predictor',\n",
    "        'Dummy_strat': dummy + ' predictor,\\nstratified',\n",
    "        'LinearReg': 'Linear Regression',\n",
    "        'LogisticReg': 'Logistic Regression',\n",
    "        'RandomForest': 'Random Forest',\n",
    "        'NaiveBayes': 'Naive Bayes',\n",
    "        'SVM_linear': 'SVM: linear',\n",
    "        'SVM_rbf': 'SVM: rbf',\n",
    "        'KNN': 'K-NN',\n",
    "        'MLP': 'MLP'\n",
    "    }\n",
    "    \n",
    "    df.columns = [name_changes_cols[name] for name in df.columns]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load results obtained from running this as a .py script with n_jobs=-1\n",
    "if load_file:\n",
    "    models = pickle.load(open( RESULTS_PATH + load_file, \"rb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without variable 'red' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no results: LinearReg\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Majority predictor</th>\n",
       "      <th>Majority predictor,\n",
       "stratified</th>\n",
       "      <th>K-NN</th>\n",
       "      <th>Logistic Regression</th>\n",
       "      <th>MLP</th>\n",
       "      <th>Naive Bayes</th>\n",
       "      <th>Random Forest</th>\n",
       "      <th>SVM: linear</th>\n",
       "      <th>SVM: rbf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F1-score, macro</th>\n",
       "      <td>0.086</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision, macro</th>\n",
       "      <td>0.061</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.454</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.583</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall, macro</th>\n",
       "      <td>0.143</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.369</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.428 $\\pm$ 0.022</td>\n",
       "      <td>0.325 $\\pm$ 0.021</td>\n",
       "      <td>0.614 $\\pm$ 0.022</td>\n",
       "      <td>0.524 $\\pm$ 0.022</td>\n",
       "      <td>0.467 $\\pm$ 0.022</td>\n",
       "      <td>0.397 $\\pm$ 0.022</td>\n",
       "      <td>0.694 $\\pm$ 0.021</td>\n",
       "      <td>0.519 $\\pm$ 0.022</td>\n",
       "      <td>0.606 $\\pm$ 0.022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Majority predictor Majority predictor,\\nstratified  \\\n",
       "F1-score, macro               0.086                           0.143   \n",
       "Precision, macro              0.061                           0.146   \n",
       "Recall, macro                 0.143                           0.146   \n",
       "Accuracy          0.428 $\\pm$ 0.022               0.325 $\\pm$ 0.021   \n",
       "\n",
       "                               K-NN Logistic Regression                MLP  \\\n",
       "F1-score, macro               0.344               0.192              0.128   \n",
       "Precision, macro              0.454               0.222              0.139   \n",
       "Recall, macro                 0.320               0.203              0.164   \n",
       "Accuracy          0.614 $\\pm$ 0.022   0.524 $\\pm$ 0.022  0.467 $\\pm$ 0.022   \n",
       "\n",
       "                        Naive Bayes      Random Forest        SVM: linear  \\\n",
       "F1-score, macro               0.198              0.408              0.175   \n",
       "Precision, macro              0.262              0.583              0.224   \n",
       "Recall, macro                 0.273              0.369              0.195   \n",
       "Accuracy          0.397 $\\pm$ 0.022  0.694 $\\pm$ 0.021  0.519 $\\pm$ 0.022   \n",
       "\n",
       "                           SVM: rbf  \n",
       "F1-score, macro               0.333  \n",
       "Precision, macro              0.431  \n",
       "Recall, macro                 0.306  \n",
       "Accuracy          0.606 $\\pm$ 0.022  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# full table with 'fake' standard errors to get a sense of the variance\n",
    "#display(create_df(models, 'x_score_clf', prettify='std'))\n",
    "df_x_clf = create_df(models, 'x_score_clf', prettify='classification')\n",
    "df_x_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no results: LogisticReg\n",
      "no results: NaiveBayes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean predictor</th>\n",
       "      <th>Mean predictor,\n",
       "stratified</th>\n",
       "      <th>K-NN</th>\n",
       "      <th>Linear Regression</th>\n",
       "      <th>MLP</th>\n",
       "      <th>Random Forest</th>\n",
       "      <th>SVM: linear</th>\n",
       "      <th>SVM: rbf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F1-score, macro</th>\n",
       "      <td>0.086</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.336</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE</th>\n",
       "      <td>0.695</td>\n",
       "      <td>0.695</td>\n",
       "      <td>0.463</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.577</td>\n",
       "      <td>0.449</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.428 $\\pm$ 0.022</td>\n",
       "      <td>0.428 $\\pm$ 0.022</td>\n",
       "      <td>0.623 $\\pm$ 0.022</td>\n",
       "      <td>0.515 $\\pm$ 0.022</td>\n",
       "      <td>0.526 $\\pm$ 0.022</td>\n",
       "      <td>0.661 $\\pm$ 0.021</td>\n",
       "      <td>0.515 $\\pm$ 0.022</td>\n",
       "      <td>0.570 $\\pm$ 0.022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSE</th>\n",
       "      <td>0.768 $\\pm$ 0.051</td>\n",
       "      <td>0.768 $\\pm$ 0.051</td>\n",
       "      <td>0.450 $\\pm$ 0.041</td>\n",
       "      <td>0.576 $\\pm$ 0.045</td>\n",
       "      <td>0.547 $\\pm$ 0.043</td>\n",
       "      <td>0.369 $\\pm$ 0.033</td>\n",
       "      <td>0.576 $\\pm$ 0.045</td>\n",
       "      <td>0.493 $\\pm$ 0.038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Mean predictor Mean predictor,\\nstratified  \\\n",
       "F1-score, macro              0.086                       0.086   \n",
       "MAE                          0.695                       0.695   \n",
       "Accuracy         0.428 $\\pm$ 0.022           0.428 $\\pm$ 0.022   \n",
       "MSE              0.768 $\\pm$ 0.051           0.768 $\\pm$ 0.051   \n",
       "\n",
       "                              K-NN  Linear Regression                MLP  \\\n",
       "F1-score, macro              0.336              0.201              0.210   \n",
       "MAE                          0.463              0.592              0.577   \n",
       "Accuracy         0.623 $\\pm$ 0.022  0.515 $\\pm$ 0.022  0.526 $\\pm$ 0.022   \n",
       "MSE              0.450 $\\pm$ 0.041  0.576 $\\pm$ 0.045  0.547 $\\pm$ 0.043   \n",
       "\n",
       "                     Random Forest        SVM: linear           SVM: rbf  \n",
       "F1-score, macro              0.324              0.202              0.242  \n",
       "MAE                          0.449              0.592              0.530  \n",
       "Accuracy         0.661 $\\pm$ 0.021  0.515 $\\pm$ 0.022  0.570 $\\pm$ 0.022  \n",
       "MSE              0.369 $\\pm$ 0.033  0.576 $\\pm$ 0.045  0.493 $\\pm$ 0.038  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#display(create_df(models, 'x_score_reg', prettify='std'))\n",
    "df_x_reg = create_df(models, 'x_score_reg', prettify='regression')\n",
    "df_x_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Including variable 'red'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no results: LinearReg\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Majority predictor</th>\n",
       "      <th>Majority predictor,\n",
       "stratified</th>\n",
       "      <th>K-NN</th>\n",
       "      <th>Logistic Regression</th>\n",
       "      <th>MLP</th>\n",
       "      <th>Naive Bayes</th>\n",
       "      <th>Random Forest</th>\n",
       "      <th>SVM: linear</th>\n",
       "      <th>SVM: rbf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F1-score, macro</th>\n",
       "      <td>0.086</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.343</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision, macro</th>\n",
       "      <td>0.061</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.459</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.272</td>\n",
       "      <td>0.584</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall, macro</th>\n",
       "      <td>0.143</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.319</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.428 $\\pm$ 0.022</td>\n",
       "      <td>0.322 $\\pm$ 0.021</td>\n",
       "      <td>0.611 $\\pm$ 0.022</td>\n",
       "      <td>0.511 $\\pm$ 0.022</td>\n",
       "      <td>0.516 $\\pm$ 0.022</td>\n",
       "      <td>0.483 $\\pm$ 0.022</td>\n",
       "      <td>0.680 $\\pm$ 0.021</td>\n",
       "      <td>0.506 $\\pm$ 0.022</td>\n",
       "      <td>0.602 $\\pm$ 0.022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Majority predictor Majority predictor,\\nstratified  \\\n",
       "F1-score, macro               0.086                           0.141   \n",
       "Precision, macro              0.061                           0.141   \n",
       "Recall, macro                 0.143                           0.139   \n",
       "Accuracy          0.428 $\\pm$ 0.022               0.322 $\\pm$ 0.021   \n",
       "\n",
       "                               K-NN Logistic Regression                MLP  \\\n",
       "F1-score, macro               0.343               0.187              0.179   \n",
       "Precision, macro              0.459               0.218              0.178   \n",
       "Recall, macro                 0.319               0.198              0.196   \n",
       "Accuracy          0.611 $\\pm$ 0.022   0.511 $\\pm$ 0.022  0.516 $\\pm$ 0.022   \n",
       "\n",
       "                        Naive Bayes      Random Forest        SVM: linear  \\\n",
       "F1-score, macro               0.232              0.417              0.170   \n",
       "Precision, macro              0.272              0.584              0.216   \n",
       "Recall, macro                 0.235              0.384              0.190   \n",
       "Accuracy          0.483 $\\pm$ 0.022  0.680 $\\pm$ 0.021  0.506 $\\pm$ 0.022   \n",
       "\n",
       "                           SVM: rbf  \n",
       "F1-score, macro               0.334  \n",
       "Precision, macro              0.431  \n",
       "Recall, macro                 0.306  \n",
       "Accuracy          0.602 $\\pm$ 0.022  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#display(create_df(models, 'x_inclRed_score_clf', prettify='std'))\n",
    "df_x_inclRed_clf = create_df(models, 'x_inclRed_score_clf', prettify='classification')\n",
    "df_x_inclRed_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no results: LogisticReg\n",
      "no results: NaiveBayes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean predictor</th>\n",
       "      <th>Mean predictor,\n",
       "stratified</th>\n",
       "      <th>K-NN</th>\n",
       "      <th>Linear Regression</th>\n",
       "      <th>MLP</th>\n",
       "      <th>Random Forest</th>\n",
       "      <th>SVM: linear</th>\n",
       "      <th>SVM: rbf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F1-score, macro</th>\n",
       "      <td>0.086</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE</th>\n",
       "      <td>0.695</td>\n",
       "      <td>0.695</td>\n",
       "      <td>0.468</td>\n",
       "      <td>0.597</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.442</td>\n",
       "      <td>0.597</td>\n",
       "      <td>0.531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.428 $\\pm$ 0.022</td>\n",
       "      <td>0.428 $\\pm$ 0.022</td>\n",
       "      <td>0.617 $\\pm$ 0.022</td>\n",
       "      <td>0.507 $\\pm$ 0.022</td>\n",
       "      <td>0.518 $\\pm$ 0.022</td>\n",
       "      <td>0.658 $\\pm$ 0.021</td>\n",
       "      <td>0.507 $\\pm$ 0.022</td>\n",
       "      <td>0.568 $\\pm$ 0.022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSE</th>\n",
       "      <td>0.768 $\\pm$ 0.051</td>\n",
       "      <td>0.768 $\\pm$ 0.051</td>\n",
       "      <td>0.455 $\\pm$ 0.041</td>\n",
       "      <td>0.580 $\\pm$ 0.045</td>\n",
       "      <td>0.565 $\\pm$ 0.044</td>\n",
       "      <td>0.363 $\\pm$ 0.032</td>\n",
       "      <td>0.580 $\\pm$ 0.045</td>\n",
       "      <td>0.502 $\\pm$ 0.040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Mean predictor Mean predictor,\\nstratified  \\\n",
       "F1-score, macro              0.086                       0.086   \n",
       "MAE                          0.695                       0.695   \n",
       "Accuracy         0.428 $\\pm$ 0.022           0.428 $\\pm$ 0.022   \n",
       "MSE              0.768 $\\pm$ 0.051           0.768 $\\pm$ 0.051   \n",
       "\n",
       "                              K-NN  Linear Regression                MLP  \\\n",
       "F1-score, macro              0.333              0.198              0.204   \n",
       "MAE                          0.468              0.597              0.590   \n",
       "Accuracy         0.617 $\\pm$ 0.022  0.507 $\\pm$ 0.022  0.518 $\\pm$ 0.022   \n",
       "MSE              0.455 $\\pm$ 0.041  0.580 $\\pm$ 0.045  0.565 $\\pm$ 0.044   \n",
       "\n",
       "                     Random Forest        SVM: linear           SVM: rbf  \n",
       "F1-score, macro              0.333              0.198              0.243  \n",
       "MAE                          0.442              0.597              0.531  \n",
       "Accuracy         0.658 $\\pm$ 0.021  0.507 $\\pm$ 0.022  0.568 $\\pm$ 0.022  \n",
       "MSE              0.363 $\\pm$ 0.032  0.580 $\\pm$ 0.045  0.502 $\\pm$ 0.040  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#display(create_df(models, 'x_inclRed_score_reg', prettify='std'))\n",
    "df_x_inclRed_reg = create_df(models, 'x_inclRed_score_reg', prettify='regression')\n",
    "df_x_inclRed_reg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
