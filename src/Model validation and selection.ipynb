{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup, globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler, LabelBinarizer\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.svm import SVC, SVR, LinearSVR, LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, cross_validate, cross_val_predict, cross_val_score, train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "from sklearn.metrics import classification_report, make_scorer, accuracy_score, f1_score \n",
    "from sklearn.utils import resample\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_state = 42 \n",
    "# select computationally feasible options. As working with constraint resources, \n",
    "# random search is more likely to find one of the best specifications.\n",
    "N_FOLDS = 5 # 5\n",
    "N_RANDOMSEARCH = 75 # 75\n",
    "\n",
    "DATA_PATH = 'data/'\n",
    "RESULTS_PATH = 'results/'\n",
    "pd.options.display.float_format = '{:.3f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wine = pd.read_csv(DATA_PATH + 'wines_transformed.csv')\n",
    "\n",
    "y = wine['quality']\n",
    "x = wine.drop(['quality', 'red'], axis=1)\n",
    "x_inclRed = wine.drop(['quality'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "Set up models and hyperparameter space\n",
    "We were instructed to run it as a classification and regression task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clfs = {}\n",
    "clfs['RandomForest'] = {'classifier': RandomForestClassifier(),\n",
    "                         'regressor': RandomForestRegressor(),\n",
    "                         'standardise': False,\n",
    "                         'clf_1hot': False,\n",
    "                         'param_grid': \n",
    "                              {'RandomForest__n_estimators': [10, 50, 100],\n",
    "                               'RandomForest__max_features': ['auto', 'sqrt'],\n",
    "                               'RandomForest__max_depth': scipy.stats.randint(10,100),\n",
    "                               'RandomForest__min_samples_split':  scipy.stats.randint(2,12),\n",
    "                               'RandomForest__min_samples_leaf': scipy.stats.randint(1,5),\n",
    "                               #'RandomForest__class_weight': [None, 'balanced']\n",
    "                              }}\n",
    "\n",
    "clfs['LogisticReg'] = {'classifier': LogisticRegression(),\n",
    "                         'regressor': None,\n",
    "                         'standardise': True,\n",
    "                         'clf_1hot': False,\n",
    "                         'param_grid': \n",
    "                              {\n",
    "                                  'LogisticReg__class_weight': [None, 'balanced']\n",
    "                              }}\n",
    "clfs['LinearReg'] = {'classifier': None,\n",
    "                         'regressor': LinearRegression(),\n",
    "                         'standardise': True,\n",
    "                         'clf_1hot': False,\n",
    "                         'param_grid': \n",
    "                              {\n",
    "                                  'LinearReg__class_weight': [None, 'balanced']\n",
    "                              }}\n",
    "clfs['NaiveBayes'] = {'classifier': GaussianNB(),\n",
    "                         'regressor': None,\n",
    "                         'standardise': True,\n",
    "                         'clf_1hot': False,\n",
    "                         'param_grid': \n",
    "                              {}}                      \n",
    "clfs['KNN'] = {'classifier': KNeighborsClassifier(),\n",
    "                         'regressor': KNeighborsRegressor(),\n",
    "                         'standardise': True,\n",
    "                         'clf_1hot': False,\n",
    "                         'param_grid': \n",
    "                              {'KNN__n_neighbors': scipy.stats.randint(1,10),\n",
    "                               'KNN__weights': ['uniform' , 'distance']}}\n",
    "\n",
    "# define tuples for 2 or! 3 layers of 64 or 128 hidden units\n",
    "layer_sizes = [[l1, l2, l3] for l1 in [64,128] for l2 in [64,128] for l3 in [None,64,128]]\n",
    "[l.pop(-1) for l in layer_sizes if l[-1]==None]\n",
    "clfs['MLP'] = {'classifier': MLPClassifier(early_stopping=True), # reduce runtime\n",
    "                'regressor': MLPRegressor(early_stopping=True),\n",
    "                'standardise': True,\n",
    "                'clf_1hot': False,\n",
    "                'param_grid': \n",
    "                      {'MLP__hidden_layer_sizes': layer_sizes,\n",
    "                       'MLP__alpha':  scipy.stats.uniform(10**(-4), 10**3),\n",
    "                      }}\n",
    "clfs['Dummy_strat'] = {'classifier': DummyClassifier(strategy='stratified'), # default\n",
    "                         'regressor': DummyRegressor(strategy='mean'), # default\n",
    "                         'standardise': True,\n",
    "                         'clf_1hot': False,\n",
    "                         'param_grid': \n",
    "                             {}}\n",
    "clfs['Dummy_majority'] = {'classifier': DummyClassifier(strategy='most_frequent'),\n",
    "                         'regressor': DummyRegressor(strategy='mean'), # default\n",
    "                         'standardise': True,\n",
    "                         'clf_1hot': False,\n",
    "                         'param_grid': \n",
    "                             {}}\n",
    "\n",
    "# split SVM to speed up as certain paramters are kernel-specific \n",
    "cache_size = 5000 #in MB, speed-up if enough RAM (default: 200)\n",
    "C = np.logspace(-5, 15, num=30, base=2)\n",
    "clfs['SVM_rbf'] = {'classifier': SVC(kernel='rbf', cache_size=cache_size),\n",
    "            'regressor': SVR(kernel='rbf', cache_size=cache_size),\n",
    "            'standardise': True,\n",
    "            'clf_1hot': False,\n",
    "            'param_grid': \n",
    "                  {'SVM_rbf__C': C,\n",
    "                   'SVM_rbf__gamma': np.logspace(-15, 3, num=19, base=2), # only used by rbf\n",
    "                   'SVM_rbf__class_weight': [None, 'balanced']\n",
    "            }}\n",
    "\n",
    "# forgo polynomial kernel as runtime seems really exzessive\n",
    "#clfs['SVM_poly'] = {'classifier': SVC(kernel='poly', cache_size=cache_size),\n",
    "#                'regressor': SVR(kernel='poly', cache_size=cache_size),\n",
    "#                'standardise': False,\n",
    "#                'clf_1hot': False,\n",
    "#                'param_grid': \n",
    "#                      {'SVM_poly__C': C,\n",
    "#                       'SVM_poly__degree': [2,3,4], # only used by poly\n",
    "#                       'SVM_poly__class_weight': [None, 'balanced']}}\n",
    "\n",
    "# more efficient than SVC / SVR. \n",
    "# Allows to solve the primal problem, as #obs >> #features in this case\n",
    "# l2 loss, needed if dual=false\n",
    "clfs['SVM_linear'] = {'classifier': LinearSVC(dual=False),\n",
    "                'regressor': LinearSVR(dual=False, loss='squared_epsilon_insensitive'),\n",
    "                'standardise': True,\n",
    "                'clf_1hot': False,\n",
    "                'param_grid': \n",
    "                      {'SVM_linear__C': np.logspace(-5, 15, num=N_RANDOMSEARCH, base=2), # hyperparameter space needs to be large enough\n",
    "                       'SVM_linear__class_weight': [None, 'balanced']\n",
    "                        }}\n",
    "\n",
    "\n",
    "# define new scores for variance of the error (used for CIs) to not have to \n",
    "# explicitely obtain predictions via use cross_val_predict\n",
    "def mean_variance_mse(y, y_pred, **kwargs):\n",
    "    SE = (y - y_pred)**2\n",
    "    SE = SE.astype(np.float128) # seems like sklean switches to pd.var which doesn't support dtype argument and uses ddof=1. So better make sure\n",
    "    v = np.var(SE, ddof=0) / (SE.shape[0] - 1)\n",
    "    return v \n",
    "\n",
    "def mean_variance_acc(y, y_pred, **kwargs):\n",
    "    acc = (y == y_pred)\n",
    "    acc = acc.astype(np.float128) \n",
    "    v = np.var(acc, ddof=0) / (acc.shape[0] - 1)\n",
    "    return v\n",
    "\n",
    "def mean_variance_acc_reg(y, y_pred, **kwargs):\n",
    "    preds = np.round(y_pred, 0).astype(int)\n",
    "    acc = (y.astype(int) == preds)\n",
    "    acc = acc.astype(np.float128) \n",
    "    v = np.var(acc, ddof=0) / (acc.shape[0] - 1)\n",
    "    return v\n",
    "\n",
    "def regression_acc(y, y_pred, **kwargs):\n",
    "    preds = np.round(y_pred, 0).astype(int)\n",
    "    return (accuracy_score(y.astype(int), preds))\n",
    "\n",
    "def regression_f1_macro(y, y_pred, **kwargs):\n",
    "    preds = np.round(y_pred, 0).astype(int)\n",
    "    return f1_score(y.astype(int), preds, average='macro')\n",
    "\n",
    "mean_variance_score_mse = make_scorer(mean_variance_mse, greater_is_better=False)\n",
    "mean_variance_score_acc = make_scorer(mean_variance_acc, greater_is_better=True)\n",
    "mean_variance_score_acc_reg = make_scorer(mean_variance_acc_reg, greater_is_better=True)\n",
    "regression_acc_score =    make_scorer(regression_acc, greater_is_better=True)\n",
    "regression_f1_macro_score = make_scorer(regression_f1_macro, greater_is_better=True)\n",
    "\n",
    "# differentiate between classification and regression\n",
    "scoring_reg = {'mse': 'neg_mean_squared_error',\n",
    "               'mae': 'neg_mean_absolute_error',\n",
    "               'acc_reg': regression_acc_score,\n",
    "               'f1_macro': regression_f1_macro_score,\n",
    "               'var_mse': mean_variance_score_mse,\n",
    "               'var_acc': mean_variance_score_acc_reg}\n",
    "scoring_clf = {'acc': 'accuracy',\n",
    "               'precision_macro': 'precision_macro', # micro: calculated globally\n",
    "               'precision_weighted': 'precision_weighted',\n",
    "               'recall_macro': 'recall_macro',\n",
    "               'recall_weighted': 'recall_weighted',\n",
    "               'f1_micro': 'f1_micro',\n",
    "               'f1_macro': 'f1_macro',\n",
    "               'f1_weighted': 'f1_weighted',\n",
    "               'var_acc': mean_variance_score_acc}\n",
    "prefixes = ['x_', 'x_inclRed_']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a valid model validation setup:  \n",
    "An inner loop to tune the models and an outer loop to obtain the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inner_cv = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=random_state)\n",
    "outer_cv = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=random_state+1)\n",
    "\n",
    "for idx_data, X in enumerate([x, x_inclRed]): \n",
    "    print('\\nSTART NEW DATASET-LOOP\\n')\n",
    "    prefix = prefixes[idx_data]\n",
    "    \n",
    "    for classification in [True, False]:\n",
    "        if classification:\n",
    "            estimator = 'classifier'\n",
    "            scoring = scoring_clf\n",
    "            suffix = '_clf'\n",
    "        else:\n",
    "            estimator = 'regressor'\n",
    "            scoring = scoring_reg\n",
    "            suffix = '_reg'\n",
    "\n",
    "        for model, d in clfs.items():\n",
    "            print('CURRENT MODEL:', model, '- classification', classification)\n",
    "            if d[estimator] == None:\n",
    "                print('No estimator')\n",
    "                continue\n",
    "\n",
    "            # never actually needed, turns out sklearn always handles this\n",
    "            if classification and d['clf_1hot']:\n",
    "                lb = LabelBinarizer(sparse_output=False)\n",
    "                Y = lb.fit_transform(y)\n",
    "            else:\n",
    "                Y = y\n",
    "\n",
    "            pipe = Pipeline([\n",
    "                        ('scaler', RobustScaler(with_centering=d['standardise'], with_scaling=d['standardise'])),\n",
    "                        (model, d[estimator])\n",
    "            ])\n",
    "\n",
    "            # CV\n",
    "            grid = d['param_grid'].copy()\n",
    "            \n",
    "            # Benchmarks without hyperparameters: only run 1 iteration\n",
    "            if grid == {}:\n",
    "                iters = 1\n",
    "            else:\n",
    "                iters = N_RANDOMSEARCH\n",
    "               \n",
    "            # run with n_jobs=-1 to utilize all resources (outside of notebook as it needs protection by a \n",
    "            # if __name__ == \"__main__\" clause if run on windows)\n",
    "            gs = RandomizedSearchCV(pipe, grid, cv=inner_cv, n_iter=iters)\n",
    "            score = cross_validate(gs, X=X, y=Y, cv=outer_cv, scoring=scoring, return_train_score=False)\n",
    "\n",
    "            d[prefix + 'score' + suffix] = score\n",
    "\n",
    "            if classification:\n",
    "                print('Acc: {:.3f}'.format(np.mean(score['test_acc'])))\n",
    "                #print(classification_report(y, preds))\n",
    "            else: \n",
    "                print('MSE: {:.3f}'.format(- np.mean(score['test_mse'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_df(clfs, name, show_std=False):\n",
    "    '''\n",
    "    show_std: add +_196 * std of the scores, this is only to get an idea of the variance, \n",
    "    not in any way a valid confidence interval (correlated runs and way too few observations)!\n",
    "    '''\n",
    "    results = {}\n",
    "    for clf in clfs.keys():\n",
    "        try:\n",
    "            tmp = clfs[clf][name].copy()\n",
    "            for k, v in tmp.items():\n",
    "                # np.abs() because 'mse' and 'mae' are defined as their negative\n",
    "                if 'var_acc' in k:\n",
    "                    if 'test_acc' in tmp.keys():\n",
    "                        acc_var = 'test_acc'\n",
    "                    elif 'test_acc_reg' in tmp.keys():\n",
    "                        acc_var = 'test_acc_reg'\n",
    "                    tmp[k] = '{:.3f} $\\pm$ {:.3f}'.format(np.abs(np.mean(clfs[clf][name][acc_var])), \n",
    "                                                              1.96 * np.sqrt(np.mean(v)))\n",
    "                    \n",
    "                elif 'var_mse' in k:\n",
    "                    tmp[k] = '{:.3f} $\\pm$ {:.3f}'.format(np.abs(np.mean(clfs[clf][name]['test_mse'])), \n",
    "                                                          1.96 * np.sqrt(-np.mean(v)))\n",
    "                else:\n",
    "                    if show_std:\n",
    "                        tmp[k] = '{:.3f} $\\pm$ {:.3f}'.format(np.abs(np.mean(v)), 1.96*np.std(v))\n",
    "                    else: \n",
    "                        tmp[k] = np.abs(np.mean(v))\n",
    "            results[clf] = tmp\n",
    "        except:\n",
    "            print('no results:', clf)\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def prettify_table(df, regression=False):    \n",
    "    name_changes_idx = {\n",
    "        #'test_f1_micro': 'F1-score, micro',\n",
    "        'test_f1_macro': 'F1-score, macro',\n",
    "        #'test_f1_weighted': 'F1-score, weighted',\n",
    "        'test_precision_macro': 'Precision, macro',\n",
    "        #'test_precision_weighted': 'Precision, weighted',\n",
    "        'test_recall_macro': 'Recall, macro',\n",
    "        #'test_recall_weighted': 'Recall, weighted',\n",
    "        'test_var_acc': 'Accuracy',\n",
    "        \n",
    "        #'test_acc_reg': 'Accuracy',\n",
    "        'test_mae': 'MAE',\n",
    "        'test_var_mse': 'MSE'\n",
    "    }\n",
    "    drops = [name for name in df.index if name not in name_changes_idx.keys()]\n",
    "    df = df.drop(drops, axis=0)\n",
    "    \n",
    "    df.index = [name_changes_idx[name] for name in df.index]\n",
    "    \n",
    "    if regression: dummy = 'Mean'\n",
    "    else: dummy = 'Majority'\n",
    "    name_changes_cols = {\n",
    "        'Dummy_majority': dummy + ' predictor',\n",
    "        'Dummy_strat': dummy + ' predictor,\\nstratified',\n",
    "        'LinearReg': 'Linear Regression',\n",
    "        'LogisticReg': 'Logistic Regression',\n",
    "        'RandomForest': 'Random Forest',\n",
    "        'NaiveBayes': 'Naive Bayes',\n",
    "        'SVM_linear': 'SVM: linear',\n",
    "        'SVM_rbf': 'SVM: rbf',\n",
    "        'KNN': 'K-NN',\n",
    "        'MLP': 'MLP'\n",
    "    }\n",
    "    \n",
    "    df.columns = [name_changes_cols[name] for name in df.columns]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "data type \"f16\" not understood",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-67a592fa123d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# load results obtained from running this as a .py script with n_jobs=-1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mclfs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mRESULTS_PATH\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'A4_results_new'\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m\".p\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: data type \"f16\" not understood"
     ]
    }
   ],
   "source": [
    "# load results obtained from running this as a .py script with n_jobs=-1\n",
    "clfs = pickle.load(open( RESULTS_PATH + 'A4_results_new' +\".p\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without 'red' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#display(create_df(clfs, 'x_score_clf', show_std=True))\n",
    "df_x_clf = prettify_table(create_df(clfs, 'x_score_clf', show_std=False))\n",
    "df_x_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Regression\n",
    "Note that mse and mae are the negative!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#display(create_df(clfs, 'x_score_reg', show_std=True))\n",
    "df_x_reg = prettify_table(create_df(clfs, 'x_score_reg', show_std=False))\n",
    "df_x_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Including Red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#display(create_df(clfs, 'x_inclRed_score_clf', show_std=True))\n",
    "df_x_inclRed_clf = prettify_table(create_df(clfs, 'x_inclRed_score_clf', show_std=False))\n",
    "df_x_inclRed_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#display(create_df(clfs, 'x_inclRed_score_reg', show_std=True))\n",
    "df_x_inclRed_reg = prettify_table(create_df(clfs, 'x_inclRed_score_reg', show_std=False))\n",
    "df_x_inclRed_reg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
